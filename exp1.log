Microsoft Windows [版本 10.0.26100.2454]
(c) Microsoft Corporation。保留所有权利。

(QMR_CV) C:\Users\Administrator\Desktop\CV>cd lab1

(QMR_CV) C:\Users\Administrator\Desktop\CV\lab1>python lab.py
timer 1s
start
IF GPU READY: True
cuda
Traceback (most recent call last):
  File "C:\Users\Administrator\Desktop\CV\lab1\lab.py", line 61, in <module>
    inputs, labels = inputs.to(device), labels.to(device)
                                        ^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'to'

(QMR_CV) C:\Users\Administrator\Desktop\CV\lab1>python lab.py
timer 1s
start
IF GPU READY: True
cuda
C:\Users\Administrator\Desktop\CV\lab1\lab.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  inputs, labels = torch.tensor(inputs).to(device), torch.tensor(labels).to(device)
Traceback (most recent call last):
  File "C:\Users\Administrator\Desktop\CV\lab1\lab.py", line 65, in <module>
    loss = criterion(outputs, labels)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\loss.py", line 1293, in forward
    return F.cross_entropy(
           ^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\functional.py", line 3479, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Expected input batch_size (4) to match target batch_size (3999).

(QMR_CV) C:\Users\Administrator\Desktop\CV\lab1>python lab.py
timer 1s
start
IF GPU READY: True
cuda
Traceback (most recent call last):
  File "C:\Users\Administrator\Desktop\CV\lab1\lab.py", line 61, in <module>
    inputs, labels = inputs.to(device), labels.to(device)
                                        ^^^^^^^^^
AttributeError: 'numpy.ndarray' object has no attribute 'to'

(QMR_CV) C:\Users\Administrator\Desktop\CV\lab1>python lab.py
timer 1s
start
IF GPU READY: True
cuda
C:\Users\Administrator\Desktop\CV\lab1\lab.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  inputs, labels = torch.tensor(inputs).to(device), torch.tensor(labels).to(device)
Traceback (most recent call last):
  File "C:\Users\Administrator\Desktop\CV\lab1\lab.py", line 65, in <module>
    loss = criterion(outputs, labels)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\loss.py", line 1293, in forward
    return F.cross_entropy(
           ^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\functional.py", line 3479, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Expected input batch_size (3599) to match target batch_size (3999).

(QMR_CV) C:\Users\Administrator\Desktop\CV\lab1>python lab.py
timer 1s
start
IF GPU READY: True
cuda
C:\Users\Administrator\Desktop\CV\lab1\lab.py:61: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  inputs, labels = torch.tensor(inputs).to(device), torch.tensor(labels).to(device)
Traceback (most recent call last):
  File "C:\Users\Administrator\Desktop\CV\lab1\lab.py", line 65, in <module>
    loss = criterion(outputs, labels)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\loss.py", line 1293, in forward
    return F.cross_entropy(
           ^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\functional.py", line 3479, in cross_entropy
    return torch._C._nn.cross_entropy_loss(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
ValueError: Expected input batch_size (3599) to match target batch_size (3999).

(QMR_CV) C:\Users\Administrator\Desktop\CV\lab1>python lab.py
timer 1s
start
IF GPU READY: True
cuda
Epoch [1/1000], Loss: 0.43495383858680725
Epoch [2/1000], Loss: 0.1252753585577011
Epoch [3/1000], Loss: 0.09898000955581665
Epoch [4/1000], Loss: 0.04289599135518074
Epoch [5/1000], Loss: 0.04838382825255394
Epoch [6/1000], Loss: 0.3383536636829376
Epoch [7/1000], Loss: 0.669242799282074
Epoch [8/1000], Loss: 0.00785867776721716
Epoch [9/1000], Loss: 0.020429329946637154
Epoch [10/1000], Loss: 0.11201898008584976
Epoch [11/1000], Loss: 0.010556311346590519
Epoch [12/1000], Loss: 0.006589628756046295
Epoch [13/1000], Loss: 0.02782469056546688
Epoch [14/1000], Loss: 0.12472499161958694
Epoch [15/1000], Loss: 0.0013200318207964301
Epoch [16/1000], Loss: 0.004351520445197821
Epoch [17/1000], Loss: 0.01603573001921177
Epoch [18/1000], Loss: 0.0013222318375483155
Epoch [19/1000], Loss: 0.030724259093403816
Epoch [20/1000], Loss: 0.00717395031824708
Epoch [21/1000], Loss: 0.00022500490013044327
Epoch [22/1000], Loss: 0.0003578431496862322
Epoch [23/1000], Loss: 0.00021227785327937454
Epoch [24/1000], Loss: 0.015896810218691826
Epoch [25/1000], Loss: 0.0010087455157190561
Epoch [26/1000], Loss: 0.0052026729099452496
Epoch [27/1000], Loss: 0.08466660231351852
Epoch [28/1000], Loss: 0.042036596685647964
Epoch [29/1000], Loss: 0.00042970976210199296
Epoch [30/1000], Loss: 0.005196899641305208
Epoch [31/1000], Loss: 0.0010215904330834746
Epoch [32/1000], Loss: 0.003039306728169322
Epoch [33/1000], Loss: 0.0007977935019880533
Epoch [34/1000], Loss: 0.0007186562870629132
Epoch [35/1000], Loss: 0.09320303052663803
Epoch [36/1000], Loss: 0.5045337080955505
Epoch [37/1000], Loss: 0.020610524341464043
Epoch [38/1000], Loss: 0.00030253492877818644
Epoch [39/1000], Loss: 0.0006897569983266294
Epoch [40/1000], Loss: 0.001257840427570045
Epoch [41/1000], Loss: 0.031645726412534714
Epoch [42/1000], Loss: 0.010648957453668118
Epoch [43/1000], Loss: 0.001883618999272585
Epoch [44/1000], Loss: 0.00046964912326075137
Epoch [45/1000], Loss: 0.0013525442918762565
Epoch [46/1000], Loss: 0.0015362557023763657
Epoch [47/1000], Loss: 0.003612457774579525
Epoch [48/1000], Loss: 0.045838307589292526
Epoch [49/1000], Loss: 0.0045090327039361
Epoch [50/1000], Loss: 0.006343264132738113
Epoch [51/1000], Loss: 0.00011435092892497778
Epoch [52/1000], Loss: 0.005732732359319925
Epoch [53/1000], Loss: 0.0383780412375927
Epoch [54/1000], Loss: 0.668971061706543
Epoch [55/1000], Loss: 0.005878608673810959
Epoch [56/1000], Loss: 6.29007481620647e-05
Epoch [57/1000], Loss: 0.0014235234120860696
Epoch [58/1000], Loss: 0.1637076437473297
Epoch [59/1000], Loss: 0.24278712272644043
Epoch [60/1000], Loss: 3.3417760278098285e-05
Epoch [61/1000], Loss: 0.00022057561727706343
Epoch [62/1000], Loss: 7.358635048149154e-05
Epoch [63/1000], Loss: 0.026613300666213036
Epoch [64/1000], Loss: 0.1274430900812149
Epoch [65/1000], Loss: 0.0330190546810627
Epoch [66/1000], Loss: 0.00011836758494609967
Epoch [67/1000], Loss: 0.00020393422164488584
Epoch [68/1000], Loss: 0.0012268947903066874
Epoch [69/1000], Loss: 0.014262168668210506
Epoch [70/1000], Loss: 0.001289315870963037
Epoch [71/1000], Loss: 0.0007480738568119705
Epoch [72/1000], Loss: 0.0002810764708556235
Epoch [73/1000], Loss: 0.9124124646186829
Epoch [74/1000], Loss: 0.08236091583967209
Epoch [75/1000], Loss: 0.0037783498410135508
Epoch [76/1000], Loss: 0.009281900711357594
Epoch [77/1000], Loss: 1.3629428394779097e-05
Epoch [78/1000], Loss: 3.9020389522193e-05
Epoch [79/1000], Loss: 0.001457985956221819
Epoch [80/1000], Loss: 0.00015960593009367585
Epoch [81/1000], Loss: 0.0017640216974541545
Epoch [82/1000], Loss: 0.0007292294758372009
Epoch [83/1000], Loss: 0.003912897780537605
Epoch [84/1000], Loss: 4.0251517930300906e-05
Epoch [85/1000], Loss: 0.001086671371012926
Epoch [86/1000], Loss: 0.0022698824759572744
Epoch [87/1000], Loss: 0.005520978476852179
Epoch [88/1000], Loss: 0.007048042956739664
Epoch [89/1000], Loss: 0.0037846919149160385
Epoch [90/1000], Loss: 0.008828727528452873
Epoch [91/1000], Loss: 0.0014429489383473992
Epoch [92/1000], Loss: 0.014812071807682514
Epoch [93/1000], Loss: 0.0018428023904561996
Epoch [94/1000], Loss: 0.01058374997228384
Epoch [95/1000], Loss: 0.01243149396032095
Epoch [96/1000], Loss: 0.0003403575683478266
Epoch [97/1000], Loss: 0.0005215359851717949
Epoch [98/1000], Loss: 8.674029959365726e-05
Epoch [99/1000], Loss: 4.080787766724825e-05
Epoch [100/1000], Loss: 0.040742237120866776
Epoch [101/1000], Loss: 0.0018714270554482937
Epoch [102/1000], Loss: 0.00019200885435566306
Epoch [103/1000], Loss: 0.0027741941157728434
Epoch [104/1000], Loss: 0.2494957447052002
Epoch [105/1000], Loss: 0.002138130133971572
Epoch [106/1000], Loss: 0.00017408072017133236
Epoch [107/1000], Loss: 0.0016159970546141267
Epoch [108/1000], Loss: 0.008495667017996311
Epoch [109/1000], Loss: 0.017595022916793823
Epoch [110/1000], Loss: 0.025457531213760376
Epoch [111/1000], Loss: 0.0023188882041722536
Epoch [112/1000], Loss: 0.0003011543594766408
Epoch [113/1000], Loss: 0.0028676073998212814
Epoch [114/1000], Loss: 0.13597817718982697
Epoch [115/1000], Loss: 0.0021042798180133104
Epoch [116/1000], Loss: 0.12563519179821014
Epoch [117/1000], Loss: 0.006901187356561422
Epoch [118/1000], Loss: 0.020360907539725304
Epoch [119/1000], Loss: 0.04367561265826225
Epoch [120/1000], Loss: 0.0016383221372961998
Epoch [121/1000], Loss: 0.001044831587933004
Epoch [122/1000], Loss: 0.0019287107279524207
Epoch [123/1000], Loss: 0.00011995265958830714
Epoch [124/1000], Loss: 0.0030086322221904993
Epoch [125/1000], Loss: 2.344442918911227e-06
Epoch [126/1000], Loss: 0.00015881203580647707
Epoch [127/1000], Loss: 0.00024276426120195538
Epoch [128/1000], Loss: 3.456949343672022e-05
Epoch [129/1000], Loss: 0.008919662795960903
Epoch [130/1000], Loss: 0.03829440101981163
Epoch [131/1000], Loss: 2.9881071895943023e-05
Epoch [132/1000], Loss: 0.0002864050620701164
Epoch [133/1000], Loss: 0.022332338616251945
Epoch [134/1000], Loss: 0.0010531682055443525
Epoch [135/1000], Loss: 0.007039290387183428
Epoch [136/1000], Loss: 0.0005895384238101542
Epoch [137/1000], Loss: 0.02277492731809616
Epoch [138/1000], Loss: 0.00017556855164002627
Epoch [139/1000], Loss: 0.009998059831559658
Epoch [140/1000], Loss: 9.576415322953835e-06
Epoch [141/1000], Loss: 0.3055628836154938
Epoch [142/1000], Loss: 7.338939030887559e-05
Epoch [143/1000], Loss: 0.0013202006230130792
Epoch [144/1000], Loss: 0.043346624821424484
Epoch [145/1000], Loss: 0.008277652785182
Epoch [146/1000], Loss: 0.18693000078201294
Epoch [147/1000], Loss: 4.625069777830504e-05
Epoch [148/1000], Loss: 4.776152127305977e-05
Epoch [149/1000], Loss: 0.0011840916704386473
Epoch [150/1000], Loss: 0.2518516182899475
Epoch [151/1000], Loss: 0.00021690061839763075
Epoch [152/1000], Loss: 1.784140658855904e-05
Epoch [153/1000], Loss: 0.15454904735088348
Epoch [154/1000], Loss: 0.009522702544927597
Epoch [155/1000], Loss: 0.003997707273811102
Epoch [156/1000], Loss: 0.00044255773536860943
Epoch [157/1000], Loss: 0.00010183701670030132
Epoch [158/1000], Loss: 0.002473141299560666
Epoch [159/1000], Loss: 0.0006209924467839301
Epoch [160/1000], Loss: 0.005669170990586281
Epoch [161/1000], Loss: 0.00047127451398409903
Epoch [162/1000], Loss: 0.00011987132165813819
Epoch [163/1000], Loss: 0.01630178652703762
Epoch [164/1000], Loss: 0.0009275480988435447
Epoch [165/1000], Loss: 0.04722842574119568
Epoch [166/1000], Loss: 0.0005004527629353106
Epoch [167/1000], Loss: 9.471992234466597e-05
Epoch [168/1000], Loss: 0.0015362986596301198
Epoch [169/1000], Loss: 0.0004388008965179324
Epoch [170/1000], Loss: 5.642540145345265e-06
Epoch [171/1000], Loss: 0.0006143388454802334
Epoch [172/1000], Loss: 0.0012116795405745506
Epoch [173/1000], Loss: 6.318052328424528e-06
Epoch [174/1000], Loss: 5.701751433662139e-05
Epoch [175/1000], Loss: 0.000251179764745757
Epoch [176/1000], Loss: 1.943056849995628e-05
Epoch [177/1000], Loss: 5.217192301643081e-05
Epoch [178/1000], Loss: 0.06980284303426743
Epoch [179/1000], Loss: 0.6061694025993347
Epoch [180/1000], Loss: 7.910699787316844e-05
Epoch [181/1000], Loss: 0.8999428749084473
Epoch [182/1000], Loss: 0.0015431344509124756
Epoch [183/1000], Loss: 0.00048413980402983725
Epoch [184/1000], Loss: 0.001711640041321516
Epoch [185/1000], Loss: 0.00033668300602585077
Epoch [186/1000], Loss: 0.08143544942140579
Epoch [187/1000], Loss: 4.589346281136386e-05
Epoch [188/1000], Loss: 0.0001642335409997031
Epoch [189/1000], Loss: 0.01826939545571804
Epoch [190/1000], Loss: 0.003912558313459158
Epoch [191/1000], Loss: 0.002055732300505042
Epoch [192/1000], Loss: 0.018666947260499
Epoch [193/1000], Loss: 0.03998282179236412
Epoch [194/1000], Loss: 0.003459128551185131
Epoch [195/1000], Loss: 0.007804290857166052
Epoch [196/1000], Loss: 0.003949325997382402
Epoch [197/1000], Loss: 0.010333920828998089
Epoch [198/1000], Loss: 0.0025270082987844944
Epoch [199/1000], Loss: 0.00010441386257298291
Epoch [200/1000], Loss: 0.0006989555549807847
Epoch [201/1000], Loss: 0.013403504155576229
Epoch [202/1000], Loss: 3.576142989913933e-05
Epoch [203/1000], Loss: 0.004903531167656183
Epoch [204/1000], Loss: 0.00018069666111841798
Epoch [205/1000], Loss: 5.0900114729302004e-05
Epoch [206/1000], Loss: 0.02559501864016056
Epoch [207/1000], Loss: 0.003082460956647992
Epoch [208/1000], Loss: 1.680833702266682e-05
Epoch [209/1000], Loss: 0.0003366648161318153
Epoch [210/1000], Loss: 0.0070828101597726345
Epoch [211/1000], Loss: 0.052668530493974686
Epoch [212/1000], Loss: 0.0020989973563700914
Epoch [213/1000], Loss: 0.00011998503032373264
Epoch [214/1000], Loss: 0.015271606855094433
Epoch [215/1000], Loss: 0.0049392045475542545
Epoch [216/1000], Loss: 0.00016111477452795953
Epoch [217/1000], Loss: 0.0012649690033867955
Epoch [218/1000], Loss: 4.633116259356029e-05
Epoch [219/1000], Loss: 5.419662920758128e-05
Epoch [220/1000], Loss: 0.0007031368440948427
Epoch [221/1000], Loss: 0.00438720965757966
Epoch [222/1000], Loss: 0.0004590859462041408
Epoch [223/1000], Loss: 0.0001187185916933231
Epoch [224/1000], Loss: 0.00010942292283289135
Epoch [225/1000], Loss: 0.15309756994247437
Epoch [226/1000], Loss: 0.12165316939353943
Epoch [227/1000], Loss: 0.000604152912274003
Epoch [228/1000], Loss: 0.17232732474803925
Epoch [229/1000], Loss: 0.000857457984238863
Epoch [230/1000], Loss: 0.04528741538524628
Epoch [231/1000], Loss: 0.12963519990444183
Epoch [232/1000], Loss: 0.0007794878911226988
Epoch [233/1000], Loss: 0.0003443397581577301
Epoch [234/1000], Loss: 0.0025003214832395315
Epoch [235/1000], Loss: 0.13308213651180267
Epoch [236/1000], Loss: 0.26345351338386536
Epoch [237/1000], Loss: 1.3748668607149739e-05
Epoch [238/1000], Loss: 0.001692633144557476
Epoch [239/1000], Loss: 9.496942766418215e-06
Epoch [240/1000], Loss: 0.00016476225573569536
Epoch [241/1000], Loss: 0.00032190425554290414
Epoch [242/1000], Loss: 0.000348971487255767
Epoch [243/1000], Loss: 0.0007447866373695433
Epoch [244/1000], Loss: 0.00016869565297383815
Epoch [245/1000], Loss: 0.0026510946918278933
Epoch [246/1000], Loss: 0.00025664016720838845
Epoch [247/1000], Loss: 0.0186990424990654
Epoch [248/1000], Loss: 0.005083231721073389
Epoch [249/1000], Loss: 0.1983913779258728
Epoch [250/1000], Loss: 0.011367648839950562
Epoch [251/1000], Loss: 0.11098421365022659
Epoch [252/1000], Loss: 0.0019648137968033552
Epoch [253/1000], Loss: 0.00010401498730061576
Epoch [254/1000], Loss: 0.0012662630761042237
Epoch [255/1000], Loss: 0.0015655405586585402
Epoch [256/1000], Loss: 0.26185038685798645
Epoch [257/1000], Loss: 0.0016495268791913986
Epoch [258/1000], Loss: 0.03976323828101158
Epoch [259/1000], Loss: 0.0018206970999017358
Epoch [260/1000], Loss: 0.0014550498453900218
Epoch [261/1000], Loss: 0.12985791265964508
Epoch [262/1000], Loss: 7.98697965365136e-06
Epoch [263/1000], Loss: 0.02159888856112957
Epoch [264/1000], Loss: 0.011135159991681576
Epoch [265/1000], Loss: 0.00437950249761343
Epoch [266/1000], Loss: 0.0004725938197225332
Epoch [267/1000], Loss: 5.8014816204376984e-06
Epoch [268/1000], Loss: 0.268361359834671
Epoch [269/1000], Loss: 0.004129732493311167
Epoch [270/1000], Loss: 3.3775897918530973e-06
Epoch [271/1000], Loss: 2.2291513232630678e-05
Epoch [272/1000], Loss: 0.0007389441598206758
Epoch [273/1000], Loss: 0.00017219274013768882
Epoch [274/1000], Loss: 1.573549525346607e-05
Epoch [275/1000], Loss: 3.246402047807351e-05
Epoch [276/1000], Loss: 4.927297140966402e-06
Epoch [277/1000], Loss: 0.0006502466858364642
Epoch [278/1000], Loss: 0.9389085173606873
Epoch [279/1000], Loss: 0.0005758660845458508
Epoch [280/1000], Loss: 0.000510109297465533
Epoch [281/1000], Loss: 0.005502698477357626
Epoch [282/1000], Loss: 1.0609550372464582e-05
Epoch [283/1000], Loss: 0.0008327783434651792
Epoch [284/1000], Loss: 0.0007309298380278051
Epoch [285/1000], Loss: 0.0010387097718194127
Epoch [286/1000], Loss: 2.6861018341151066e-05
Epoch [287/1000], Loss: 0.001372645259834826
Epoch [288/1000], Loss: 2.1536856365855783e-05
Epoch [289/1000], Loss: 2.407984902674798e-05
Epoch [290/1000], Loss: 1.4424171240534633e-05
Epoch [291/1000], Loss: 0.00033616137807257473
Epoch [292/1000], Loss: 0.001534531475044787
Epoch [293/1000], Loss: 6.194374873302877e-05
Epoch [294/1000], Loss: 6.862032023491338e-05
Epoch [295/1000], Loss: 0.0016684256261214614
Epoch [296/1000], Loss: 0.002257888438180089
Epoch [297/1000], Loss: 0.0033312449231743813
Epoch [298/1000], Loss: 0.11227668076753616
Epoch [299/1000], Loss: 0.0840858593583107
Epoch [300/1000], Loss: 0.021000059321522713
Epoch [301/1000], Loss: 0.0069240424782037735
Epoch [302/1000], Loss: 0.08934066444635391
Epoch [303/1000], Loss: 5.908537423238158e-05
Epoch [304/1000], Loss: 0.006029652897268534
Epoch [305/1000], Loss: 3.3099695428973064e-05
Epoch [306/1000], Loss: 0.00048349486314691603
Epoch [307/1000], Loss: 7.453806028934196e-05
Epoch [308/1000], Loss: 0.00022191095922607929
Epoch [309/1000], Loss: 0.0024303009267896414
Epoch [310/1000], Loss: 0.0003292943583801389
Epoch [311/1000], Loss: 0.0006432586233131588
Epoch [312/1000], Loss: 3.0199614684534026e-06
Epoch [313/1000], Loss: 0.010395828634500504
Epoch [314/1000], Loss: 3.754940189537592e-05
Epoch [315/1000], Loss: 0.007289662957191467
Epoch [316/1000], Loss: 3.7271787732606754e-05
Epoch [317/1000], Loss: 7.74415020714514e-05
Epoch [318/1000], Loss: 0.010665345937013626
Epoch [319/1000], Loss: 0.0010272881481796503
Epoch [320/1000], Loss: 0.0001286421320401132
Epoch [321/1000], Loss: 3.933896550734062e-06
Epoch [322/1000], Loss: 0.008076244033873081
Epoch [323/1000], Loss: 0.000923442654311657
Epoch [324/1000], Loss: 7.994045154191554e-05
Epoch [325/1000], Loss: 0.00046114236465655267
Epoch [326/1000], Loss: 0.21892844140529633
Epoch [327/1000], Loss: 0.012342195026576519
Epoch [328/1000], Loss: 0.0003089321544393897
Epoch [329/1000], Loss: 0.0015372439520433545
Epoch [330/1000], Loss: 0.0003504924534354359
Epoch [331/1000], Loss: 0.0011787355178967118
Epoch [332/1000], Loss: 0.001839496660977602
Epoch [333/1000], Loss: 0.01655675657093525
Epoch [334/1000], Loss: 0.0030320174992084503
Epoch [335/1000], Loss: 9.440704161534086e-05
Epoch [336/1000], Loss: 0.010311472229659557
Epoch [337/1000], Loss: 0.012767571024596691
Epoch [338/1000], Loss: 5.936153684160672e-05
Epoch [339/1000], Loss: 0.0026790809351950884
Epoch [340/1000], Loss: 0.0004745274782180786
Epoch [341/1000], Loss: 0.0004303473688196391
Epoch [342/1000], Loss: 0.002474934561178088
Epoch [343/1000], Loss: 0.0009375668596476316
Epoch [344/1000], Loss: 0.00491379713639617
Epoch [345/1000], Loss: 7.3114661063300446e-06
Epoch [346/1000], Loss: 0.0006540194153785706
Epoch [347/1000], Loss: 7.136110070860013e-05
Epoch [348/1000], Loss: 0.0002034210629062727
Epoch [349/1000], Loss: 0.002052210969850421
Epoch [350/1000], Loss: 0.02022840455174446
Epoch [351/1000], Loss: 0.011605030857026577
Epoch [352/1000], Loss: 9.15427808649838e-05
Epoch [353/1000], Loss: 0.20685209333896637
Epoch [354/1000], Loss: 9.05103879631497e-05
Epoch [355/1000], Loss: 6.55192052363418e-05
Epoch [356/1000], Loss: 0.08729320764541626
Epoch [357/1000], Loss: 5.077963578514755e-05
Epoch [358/1000], Loss: 7.073050710459938e-06
Epoch [359/1000], Loss: 0.022623715922236443
Epoch [360/1000], Loss: 5.737541141570546e-05
Epoch [361/1000], Loss: 0.00016119102656375617
Epoch [362/1000], Loss: 0.0006912908866070211
Epoch [363/1000], Loss: 0.0006947782239876688
Epoch [364/1000], Loss: 0.00041459998465143144
Epoch [365/1000], Loss: 0.0064354147762060165
Epoch [366/1000], Loss: 1.629167491046246e-05
Epoch [367/1000], Loss: 4.104586332687177e-05
Epoch [368/1000], Loss: 0.0004372446273919195
Epoch [369/1000], Loss: 9.237923222826794e-05
Epoch [370/1000], Loss: 0.19805043935775757
Epoch [371/1000], Loss: 0.00038864664384163916
Epoch [372/1000], Loss: 0.0023833520244807005
Epoch [373/1000], Loss: 8.741964848013595e-06
Epoch [374/1000], Loss: 0.034957095980644226
Epoch [375/1000], Loss: 1.3867732377548236e-05
Epoch [376/1000], Loss: 1.0251890671497677e-05
Epoch [377/1000], Loss: 7.74852651375113e-06
Epoch [378/1000], Loss: 0.00026867075939662755
Epoch [379/1000], Loss: 1.0331464181945194e-06
Epoch [380/1000], Loss: 0.0004906977992504835
Epoch [381/1000], Loss: 0.0003367715689819306
Epoch [382/1000], Loss: 0.0008287276141345501
Epoch [383/1000], Loss: 1.2906352281570435
Epoch [384/1000], Loss: 0.08731762319803238
Epoch [385/1000], Loss: 0.013385293073952198
Epoch [386/1000], Loss: 0.000393064838135615
Epoch [387/1000], Loss: 0.12880836427211761
Epoch [388/1000], Loss: 0.00012768949090968817
Epoch [389/1000], Loss: 0.001460521831177175
Epoch [390/1000], Loss: 0.0006337238592095673
Epoch [391/1000], Loss: 0.00013938006304670125
Traceback (most recent call last):
  File "C:\Users\Administrator\Desktop\CV\lab1\lab.py", line 64, in <module>
    outputs = model(inputs)
              ^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\Administrator\Desktop\CV\lab1\lab.py", line 43, in forward
    out = self.fc1(x)
          ^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\ProgramData\anaconda3\envs\QMR_CV\Lib\site-packages\torch\nn\modules\linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
^C
(QMR_CV) C:\Users\Administrator\Desktop\CV\lab1>python lab.py
timer 1s
start
IF GPU READY: True
cuda
Epoch [1/20], Loss: 0.557312548160553
Epoch [2/20], Loss: 0.12574650347232819
Epoch [3/20], Loss: 0.07317594438791275
Epoch [4/20], Loss: 0.14396382868289948
Epoch [5/20], Loss: 0.020471513271331787
Epoch [6/20], Loss: 0.027904100716114044
Epoch [7/20], Loss: 0.28319939970970154
Epoch [8/20], Loss: 0.08314955979585648
Epoch [9/20], Loss: 0.042805805802345276
Epoch [10/20], Loss: 0.0068653654307127
Epoch [11/20], Loss: 0.0026914107147604227
Epoch [12/20], Loss: 0.004118998069316149
Epoch [13/20], Loss: 0.005800043698400259
Epoch [14/20], Loss: 0.045344848185777664
Epoch [15/20], Loss: 0.020420856773853302
Epoch [16/20], Loss: 0.004004392307251692
Epoch [17/20], Loss: 0.001819909899495542
Epoch [18/20], Loss: 0.00039461880805902183
Epoch [19/20], Loss: 0.02185303531587124
Epoch [20/20], Loss: 0.003414002014324069
Accuracy of the model on the test set: 97.5%

(QMR_CV) C:\Users\Administrator\Desktop\CV\lab1>python lab.py
timer 1s
start
IF GPU READY: True
cuda
Epoch [1/20], Loss: 0.9791129231452942
Epoch [2/20], Loss: 0.7095702886581421
Epoch [3/20], Loss: 0.5840241312980652
Epoch [4/20], Loss: 0.5037593841552734
Epoch [5/20], Loss: 0.2824609577655792
Epoch [6/20], Loss: 0.2636409103870392
Epoch [7/20], Loss: 0.24163971841335297
Epoch [8/20], Loss: 0.17913535237312317
Epoch [9/20], Loss: 0.13728125393390656
Epoch [10/20], Loss: 0.20350486040115356
Epoch [11/20], Loss: 0.09261365234851837
Epoch [12/20], Loss: 0.12185964733362198
Epoch [13/20], Loss: 0.08304198831319809
Epoch [14/20], Loss: 0.1370508223772049
Epoch [15/20], Loss: 0.0849018469452858
Epoch [16/20], Loss: 0.14263495802879333
Epoch [17/20], Loss: 0.06403787434101105
Epoch [18/20], Loss: 0.14140333235263824
Epoch [19/20], Loss: 0.09858153015375137
Epoch [20/20], Loss: 0.03798972815275192
Accuracy of the model on the test set: 98.75%

(QMR_CV) C:\Users\Administrator\Desktop\CV\lab1>python lab.py
timer 1s
start
IF GPU READY: True
cuda
Epoch [1/40], Loss: 0.6416040062904358
Epoch [2/40], Loss: 0.18591651320457458
Epoch [3/40], Loss: 0.18794754147529602
Epoch [4/40], Loss: 0.1301601082086563
Epoch [5/40], Loss: 0.06074943020939827
Epoch [6/40], Loss: 0.03607552871108055
Epoch [7/40], Loss: 0.03371790051460266
Epoch [8/40], Loss: 0.20467939972877502
Epoch [9/40], Loss: 0.010175635106861591
Epoch [10/40], Loss: 0.3040103614330292
Epoch [11/40], Loss: 0.017325716093182564
Epoch [12/40], Loss: 0.01914740726351738
Epoch [13/40], Loss: 0.02451387420296669
Epoch [14/40], Loss: 0.06332016736268997
Epoch [15/40], Loss: 0.07292649894952774
Epoch [16/40], Loss: 0.05446407198905945
Epoch [17/40], Loss: 0.046974748373031616
Epoch [18/40], Loss: 0.02076483890414238
Epoch [19/40], Loss: 0.010959812439978123
Epoch [20/40], Loss: 0.00461134547367692
Epoch [21/40], Loss: 0.03750063106417656
Epoch [22/40], Loss: 0.005091528873890638
Epoch [23/40], Loss: 0.01347583532333374
Epoch [24/40], Loss: 0.021034391596913338
Epoch [25/40], Loss: 0.010969216004014015
Epoch [26/40], Loss: 0.026521431282162666
Epoch [27/40], Loss: 0.07454375177621841
Epoch [28/40], Loss: 0.009928323328495026
Epoch [29/40], Loss: 0.009229038842022419
Epoch [30/40], Loss: 0.03317379578948021
Epoch [31/40], Loss: 0.001816537813283503
Epoch [32/40], Loss: 0.04075228422880173
Epoch [33/40], Loss: 0.013107312843203545
Epoch [34/40], Loss: 0.11685951054096222
Epoch [35/40], Loss: 0.01647154614329338
Epoch [36/40], Loss: 0.0058609056286513805
Epoch [37/40], Loss: 0.009608314372599125
Epoch [38/40], Loss: 0.03647827357053757
Epoch [39/40], Loss: 0.02611800841987133
Epoch [40/40], Loss: 0.0005605685873888433
Accuracy of the model on the test set: 98.25%
